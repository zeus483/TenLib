# TenLib ‚Äî Estado T√©cnico del MVP

> Documento de referencia interno. Describe exactamente lo que est√° construido, c√≥mo funciona y qu√© falta. Sin aspiracionalismo.

---

## Resumen ejecutivo

TenLib es un pipeline de traducci√≥n de libros completos con IA. El MVP actual cubre **Fase 1 + Fase 3** del roadmap original: el pipeline de extremo a extremo funciona, los dos modelos (Claude y Gemini) est√°n integrados con failover autom√°tico y la reanudaci√≥n ante interrupciones est√° garantizada por hash de archivo.

**Lo que puede hacer hoy:**
```bash
tenlib translate --book libro.epub --from en --to es
```
Toma un `.epub`, `.txt` o `.md`, lo divide en chunks sem√°nticos, traduce cada uno con Claude o Gemini, y reconstruye el archivo de salida en `.txt` con reanudaci√≥n autom√°tica.

**Lo que NO est√° construido todav√≠a:** Book Bible / Context Engine, modo `fix`, modo `write`, UI Gradio, exportaci√≥n a EPUB/DOCX, Quality Checker, reset de libro ya procesado.

---

## Stack t√©cnico

| Componente       | Tecnolog√≠a                          |
|------------------|-------------------------------------|
| Lenguaje         | Python 3.11+                        |
| CLI              | Click                               |
| Storage          | SQLite3 (stdlib)                    |
| Parse EPUB       | ebooklib                            |
| Claude           | anthropic SDK ‚Äî `claude-haiku-4-5-20251001` |
| Gemini           | google-generativeai SDK ‚Äî `gemini-2.0-flash` |
| Config           | PyYAML                              |
| Tests            | pytest                              |
| Build            | setuptools ‚â• 61.0                   |
| Python m√≠nimo    | 3.11                                |

**Dependencias de producci√≥n** (`requirements.txt`):
```
click
PyYAML
python-dotenv
anthropic
google-generativeai
ebooklib
pysqlite3
```

---

## Estructura de paquetes

```
tenlib/
‚îú‚îÄ‚îÄ config.example.yaml
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ MVP.md                          ‚Üê este archivo
‚îÇ
‚îú‚îÄ‚îÄ tenlib/                         # Paquete principal (import: tenlib.xxx)
‚îÇ   ‚îú‚îÄ‚îÄ cli.py                      # Punto de entrada CLI
‚îÇ   ‚îú‚îÄ‚îÄ factory.py                  # Ensamblador de dependencias (DI manual)
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator.py             # Coordinador del pipeline
‚îÇ   ‚îú‚îÄ‚îÄ reconstructor.py            # Generador del archivo de salida
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ processor/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py               # RawBook, Chunk, ChunkStatus
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ book_processor.py       # (archivo presente, no usado en el pipeline actual)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py             # BaseParser (abstracto)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ factory.py          # ParserFactory
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ txt_parser.py       # TXT y MD
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ epub_parser.py      # EPUB via ebooklib
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chunker/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ models.py           # BoundaryType, ChunkConfig, TextSegment
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ chunker.py          # Chunker ‚Äî orquesta las dos pasadas
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ detector.py         # BoundaryDetector ‚Äî Pasada 1
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ normalizer.py       # ChunkNormalizer ‚Äî Pasada 2
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ token_estimator.py  # SimpleTokenEstimator, TikTokenEstimator
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ router/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                 # BaseModel (abstracto)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py               # ModelResponse, ModelConfig
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ router.py               # Router ‚Äî selecci√≥n y failover
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ claude.py               # ClaudeAdapter
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini.py               # GeminiAdapter
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_builder.py       # build_translate_prompt()
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ response_parser.py      # parse_model_response()
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config_loader.py        # load_model_configs() desde YAML
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ storage/
‚îÇ       ‚îú‚îÄ‚îÄ db.py                   # Conexi√≥n SQLite + schema
‚îÇ       ‚îú‚îÄ‚îÄ models.py               # StoredBook, StoredChunk, enums
‚îÇ       ‚îî‚îÄ‚îÄ repository.py           # Repository ‚Äî toda la capa de datos
‚îÇ
‚îî‚îÄ‚îÄ tests/
    ‚îú‚îÄ‚îÄ test_cli.py                 # 18 tests ‚Äî CLI
    ‚îú‚îÄ‚îÄ test_orchestrator.py        # 9 tests ‚Äî pipeline + Reconstructor
    ‚îú‚îÄ‚îÄ processor/
    ‚îÇ   ‚îú‚îÄ‚îÄ parsers/
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_parser.py      # 32 tests ‚Äî TxtParser, EpubParser, Factory
    ‚îÇ   ‚îî‚îÄ‚îÄ chunker/
    ‚îÇ       ‚îú‚îÄ‚îÄ test_detector.py    # 27 tests ‚Äî detecci√≥n de fronteras
    ‚îÇ       ‚îú‚îÄ‚îÄ test_normalizer.py  # 17 tests ‚Äî normalizaci√≥n de tokens
    ‚îÇ       ‚îî‚îÄ‚îÄ test_integration.py # 6 tests ‚Äî pipeline completo del chunker
    ‚îú‚îÄ‚îÄ router/
    ‚îÇ   ‚îú‚îÄ‚îÄ test_router.py          # 7 tests ‚Äî failover y selecci√≥n
    ‚îÇ   ‚îú‚îÄ‚îÄ test_prompt_builder.py  # 8 tests ‚Äî construcci√≥n del prompt
    ‚îÇ   ‚îî‚îÄ‚îÄ test_response_parser.py # 7 tests ‚Äî parseo resiliente de JSON
    ‚îî‚îÄ‚îÄ storage/
        ‚îî‚îÄ‚îÄ test_repository.py      # 17 tests ‚Äî CRUD SQLite

# TOTAL: 147 tests ‚Äî todos en verde
```

---

## Modelos de datos

### Enums

```python
# processor/models.py y storage/models.py
class ChunkStatus(str, Enum):
    PENDING  = "pending"    # esperando traducci√≥n
    DONE     = "done"       # confianza >= 0.75
    FLAGGED  = "flagged"    # confianza < 0.75 O error en traducci√≥n
    REVIEWED = "reviewed"   # revisi√≥n humana completada (futuro)

# processor/chunker/models.py
class BoundaryType(str, Enum):
    CHAPTER   = "chapter"   # prioridad m√°s alta
    SCENE     = "scene"
    POV       = "pov"
    PARAGRAPH = "paragraph"
    SENTENCE  = "sentence"  # prioridad m√°s baja

# storage/models.py
class BookMode(str, Enum):
    TRANSLATE = "translate"
    FIX       = "fix"       # futuro Fase 4
    WRITE     = "write"     # futuro Fase 4

class BookStatus(str, Enum):
    IN_PROGRESS = "in_progress"
    REVIEW      = "review"      # futuro
    DONE        = "done"
```

### Dataclasses de dominio

```python
# processor/models.py
@dataclass
class RawBook:
    title: str
    source_path: str
    sections: list[str]              # una entrada por secci√≥n/cap√≠tulo
    detected_language: Optional[str]

@dataclass
class Chunk:
    index: int
    original: str
    token_estimated: int
    source_section: int              # √≠ndice en RawBook.sections
    translated:   Optional[str]  = None
    model_used:   Optional[str]  = None
    confidence:   Optional[float] = None
    status:       ChunkStatus    = ChunkStatus.PENDING
    flags:        list[str]      = field(default_factory=list)

# processor/chunker/models.py
@dataclass
class TextSegment:
    text: str
    boundary_type: BoundaryType
    source_section: int
    original_position: int           # posici√≥n en caracteres en el texto original
    token_estimated: int = 0

@dataclass
class ChunkConfig:
    min_tokens:    int = 800
    max_tokens:    int = 2000
    target_tokens: int = 1400
    chapter_patterns:   list[str]    # regexes de cap√≠tulo
    scene_patterns:     list[str]    # separadores de escena
    pov_patterns:       list[str]    # marcadores de POV
    paragraph_patterns: list[str]
    sentence_patterns:  list[str]

# router/models.py
@dataclass
class ModelResponse:
    translation:   str
    confidence:    float             # [0.0 ‚Äì 1.0], clampeado
    notes:         str               # razonamiento del modelo
    model_used:    str
    tokens_input:  int
    tokens_output: int

@dataclass
class ModelConfig:
    name:               str
    priority:           int          # menor = mayor prioridad
    daily_token_limit:  int
    api_key:            Optional[str] = None
    timeout_seconds:    int   = 60
    temperature:        float = 0.3
    _unavailable_until: Optional[float] = None  # cooldown en tiempo UNIX

# storage/models.py
@dataclass
class StoredBook:
    id:          int
    title:       str
    file_hash:   str                 # SHA-256 del archivo
    mode:        BookMode
    status:      BookStatus
    created_at:  str                 # ISO timestamp
    source_lang: Optional[str]
    target_lang: Optional[str]

@dataclass
class StoredChunk:
    id:              int
    book_id:         int
    chunk_index:     int
    original:        str
    status:          ChunkStatus
    translated:      Optional[str]   = None
    model_used:      Optional[str]   = None
    confidence:      Optional[float] = None
    token_estimated: Optional[int]   = None
    source_section:  Optional[int]   = None
    flags:           list[str]       = field(default_factory=list)

# orchestrator.py
@dataclass
class PipelineResult:
    book_id:      int
    output_path:  Path
    total_chunks: int
    translated:   int                # chunks con status DONE
    flagged:      int                # chunks con status FLAGGED
    was_resumed:  bool
```

---

## Schema SQLite

Base de datos local en `~/.tenlib/tenlib.db`. Se inicializa autom√°ticamente al primer uso.

```sql
CREATE TABLE IF NOT EXISTS books (
    id          INTEGER PRIMARY KEY AUTOINCREMENT,
    title       TEXT,
    source_lang TEXT,
    target_lang TEXT,
    mode        TEXT,           -- 'translate' | 'fix' | 'write'
    status      TEXT,           -- 'in_progress' | 'review' | 'done'
    file_hash   TEXT UNIQUE,    -- SHA-256, garantiza idempotencia
    created_at  TEXT            -- ISO 8601
);

CREATE TABLE IF NOT EXISTS chunks (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    book_id         INTEGER NOT NULL,
    chunk_index     INTEGER NOT NULL,
    original        TEXT,
    translated      TEXT,
    token_estimated INTEGER,
    source_section  INTEGER,
    model_used      TEXT,
    confidence      REAL,
    status          TEXT DEFAULT 'pending',
    flags           TEXT DEFAULT '[]',   -- JSON array de strings
    UNIQUE (book_id, chunk_index),       -- idempotencia en save_chunks()
    FOREIGN KEY (book_id) REFERENCES books(id)
);

CREATE TABLE IF NOT EXISTS bible (
    id           INTEGER PRIMARY KEY AUTOINCREMENT,
    book_id      INTEGER NOT NULL,
    version      INTEGER,
    content_json TEXT,
    updated_at   TEXT,
    FOREIGN KEY (book_id) REFERENCES books(id)
);
-- Tabla reservada para Fase 2 (Context Engine). Existe en el schema pero no se escribe.

CREATE TABLE IF NOT EXISTS quota_usage (
    model       TEXT,
    date        TEXT,           -- 'YYYY-MM-DD'
    tokens_used INTEGER,
    PRIMARY KEY (model, date)
);
```

**Configuraci√≥n de conexi√≥n:**
- `row_factory = sqlite3.Row` ‚Äî acceso por nombre de columna
- `PRAGMA foreign_keys = ON`
- `PRAGMA journal_mode = WAL` ‚Äî lecturas concurrentes seguras

---

## Flujo del pipeline

```
tenlib translate --book libro.epub --from en --to es
        ‚îÇ
        ‚ñº cli.py
  _validate_file()          ‚Üí extensi√≥n en {.epub, .txt, .md} + existencia
  _validate_lang()          ‚Üí no vac√≠o, solo alfa+gui√≥n, m√°x 10 chars
  source_lang != target_lang
        ‚îÇ
        ‚ñº factory.py ‚Üí build_orchestrator()
  load_model_configs()      ‚Üí ~/.tenlib/config.yaml (o TENLIB_CONFIG_PATH)
  ClaudeAdapter + GeminiAdapter instanciados con config y repo
  Router([claude, gemini])  ‚Üí ordenados por priority
  Orchestrator(repo, ParserFactory(), Chunker(), router, Reconstructor())
        ‚îÇ
        ‚ñº orchestrator.py ‚Üí run()
  SHA-256 del archivo       ‚Üí identidad del libro (no el nombre)
  repo.get_book_by_hash()
        ‚îÇ
        ‚îú‚îÄ‚îÄ Libro nuevo:
        ‚îÇ     repo.create_book()
        ‚îÇ     ParserFactory.get_parser() ‚Üí TxtParser | EpubParser
        ‚îÇ     parser.parse()             ‚Üí RawBook{sections}
        ‚îÇ     Chunker.chunk(raw_book)    ‚Üí list[Chunk] (Pasada 1 + 2)
        ‚îÇ     repo.save_chunks()         ‚Üí INSERT OR IGNORE (idempotente)
        ‚îÇ
        ‚îî‚îÄ‚îÄ Libro existente:
              status == DONE ‚Üí BookAlreadyDoneError
              status == IN_PROGRESS ‚Üí was_resumed = True
        ‚îÇ
        ‚ñº
  repo.get_pending_chunks()  ‚Üí solo status='pending', ORDER BY chunk_index
        ‚îÇ
        ‚ñº _process_chunks() ‚Äî loop principal
  Para cada chunk pendiente:
    ‚îå‚îÄ‚îÄ try:
    ‚îÇ     Router.translate(chunk.original, system_prompt)
    ‚îÇ       ‚Üí modelos en orden de priority
    ‚îÇ       ‚Üí is_available() chequea cooldown + quota diaria
    ‚îÇ       ‚Üí si falla retryable: cooldown 5min, siguiente modelo
    ‚îÇ       ‚Üí si falla contenido (BadRequestError): re-raise sin failover
    ‚îÇ       ‚Üí si todos agotados: AllModelsExhaustedError
    ‚îÇ     repo.update_chunk_translation(status=DONE si conf‚â•0.75, FLAGGED si <0.75)
    ‚îÇ
    ‚îú‚îÄ‚îÄ except AllModelsExhaustedError:
    ‚îÇ     break  ‚Üê sale del loop, chunks restantes quedan PENDING
    ‚îÇ
    ‚îî‚îÄ‚îÄ except Exception:
          repo.flag_chunk(flags=["error: TipoError: mensaje"])
          pipeline CONTIN√öA con el siguiente chunk
        ‚îÇ
        ‚ñº
  Reconstructor.build()     ‚Üí ~/.tenlib/output/{slug}_{target_lang}.txt
  repo.update_book_status(DONE)
        ‚îÇ
        ‚ñº cli.py ‚Üí _print_summary()
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  [tenlib] ‚úì Proceso completado
  [tenlib]   Total chunks : 87
  [tenlib]   Traducidos   : 85
  [tenlib]   Flaggeados   : 2  (requieren revisi√≥n)   ‚Üê amarillo si > 0
  [tenlib]   Output       : ~/.tenlib/output/libro_es.txt
  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
```

---

## M√≥dulos en detalle

### CLI (`tenlib/cli.py`)

**Entry point:** `tenlib = tenlib.cli:main` (definido en `pyproject.toml`)

**Comandos implementados:**

| Comando     | Estado         | Descripci√≥n                          |
|-------------|----------------|--------------------------------------|
| `translate` | ‚úÖ Funcional   | Pipeline completo de traducci√≥n      |
| `fix`       | üî≤ Stub        | Imprime "Fase 4" y sale              |
| `review`    | üî≤ Stub        | Imprime "Fase 4" y sale              |
| `write`     | üî≤ Stub        | Imprime "Fase 4" y sale              |

**Validaciones de `translate`:**
- Archivo existe y es fichero regular
- Extensi√≥n en `{.epub, .txt, .md}`
- `--from` y `--to`: no vac√≠os, s√≥lo `[a-zA-Z-]`, m√°x 10 chars
- `source_lang.lower() != target_lang.lower()`
- Language codes normalizados a lowercase antes de pasar al orchestrator

**Exit codes:**
- `0` ‚Üí √©xito o KeyboardInterrupt
- `1` ‚Üí error de validaci√≥n o error inesperado
- `2` ‚Üí AllModelsExhaustedError (quota agotada)

**`_handle_already_done`:** pregunta al usuario con `click.confirm()`. Si acepta, muestra mensaje de "reset no implementado todav√≠a" ‚Äî **el reset real es deuda t√©cnica pendiente**.

---

### Orchestrator (`tenlib/orchestrator.py`)

```python
class Orchestrator:
    def __init__(self, repo, parser_factory, chunker, router, reconstructor)
    def run(self, file_path: str, source_lang: str, target_lang: str,
            mode: BookMode = BookMode.TRANSLATE) -> PipelineResult
```

**Invariantes de dise√±o:**
- Idempotente: llamarlo dos veces con el mismo archivo reanuda, no reprocesa
- No tiene l√≥gica de negocio: s√≥lo coordina m√≥dulos
- Toda excepci√≥n por chunk es capturada localmente ‚Äî el pipeline nunca aborta por un chunk individual
- `AllModelsExhaustedError` hace `break` del loop (chunks restantes quedan PENDING para el siguiente run)

**`_resolve_status(confidence)`:**
```python
return ChunkStatus.DONE if confidence >= 0.75 else ChunkStatus.FLAGGED
```

**`_build_result()`:** cuenta estado directamente desde la BD (no del loop), asegurando consistencia entre runs.

**Funciones de m√≥dulo:**
```python
def _compute_hash(path: Path) -> str:
    # SHA-256 en bloques de 64KB ‚Äî identifica el libro por contenido, no por nombre

def _slugify(title: str) -> str:
    # lowercase ‚Üí remove non-word ‚Üí spaces a underscores
    # Ejemplo: "El Nombre del Viento" ‚Üí "el_nombre_del_viento"
```

**Excepciones propias:**
```python
class BookAlreadyDoneError(Exception): ...
```

---

### Reconstructor (`tenlib/reconstructor.py`)

```python
class Reconstructor:
    def __init__(self, repo: Repository, output_dir: Path | None = None)
    def build(self, book_id: int, output_filename: str) -> Path
```

- Output por defecto: `~/.tenlib/output/`
- Chunks ordenados por `chunk_index`
- Inserta `\n\n` entre chunks de distintas secciones (`source_section` diferente)
- Une todos los chunks con `\n\n`
- Para chunks FLAGGED sin traducci√≥n: antepone `[‚ö† PENDIENTE DE REVISI√ìN]\n`
- Para chunks DONE: usa `chunk.translated`
- Para chunks PENDING sin traducci√≥n: usa `chunk.original` (fallback de seguridad)

---

### Parsers (`tenlib/processor/parsers/`)

#### TxtParser

```python
class TxtParser(BaseParser):
    def can_handle(self, file_path: str) -> bool   # .txt, .md (case-insensitive)
    def parse(self, file_path: str) -> RawBook
```

**L√≥gica de t√≠tulo:**
1. Primera l√≠nea ‚â§ 10 palabras y sin punto al final ‚Üí usarla como t√≠tulo
2. Fallback ‚Üí `Path(file_path).stem`

**L√≥gica de secciones:**
1. Contar ocurrencias de patrones de cap√≠tulo en todo el texto
2. Si hay ‚â• 2 coincidencias ‚Üí dividir por cap√≠tulos (cada match abre nueva secci√≥n)
3. Si hay < 2 ‚Üí dividir por p√°rrafos (doble salto de l√≠nea)
4. P√°rrafos con < 40 palabras se fusionan con el siguiente (evita secciones diminutas)

**Patrones de cap√≠tulo detectados:**
```python
r"(?i)^(chapter|cap√≠tulo)\s+\d+"          # Chapter 1, Cap√≠tulo 12
r"(?i)^(chapter|cap√≠tulo)\s+\w+"          # Chapter One
r"^(i{1,3}|iv|vi{0,3}|ix|x)\.$"          # I. II. III. IV. ...
r"^\d{1,2}\.$"                             # 1. 2. 12.
r"^(\*\*\*|---)$"                          # *** ---
r"^#{1,3}\s+\S"                            # ## T√≠tulo, # Cap
```

**Encoding:** UTF-8 primary, latin-1 fallback autom√°tico.

#### EpubParser

```python
class EpubParser(BaseParser):
    def can_handle(self, file_path: str) -> bool   # .epub (case-insensitive)
    def parse(self, file_path: str) -> RawBook
```

- `ebooklib` se importa lazy (dentro del m√©todo) ‚Üí `ImportError` descriptivo si no est√°
- Extrae `title` y `language` de metadatos EPUB
- Itera √≠tems del spine (orden de lectura)
- Conversi√≥n HTML ‚Üí texto plano: regex simple (no BeautifulSoup)
  - Tags de bloque (`<p>`, `<div>`, `<br>`, `<h1>`-`<h6>`) ‚Üí `\n`
  - Todas las tags eliminadas
  - Entidades HTML decodificadas: `&amp;` `&lt;` `&gt;` `&quot;` `&#39;` `&nbsp;` `&mdash;` `&ndash;` `&hellip;`
- **Descarta √≠tems con < 50 palabras** (portadas, copyright, √≠ndices)

#### ParserFactory

```python
class ParserFactory:
    def __init__(self)                           # registra EpubParser, TxtParser
    def get_parser(self, file_path: str) -> BaseParser
    def register(self, parser: BaseParser)       # inserta al frente (mayor prioridad)
    @classmethod
    def parse_file(cls, file_path: str) -> RawBook
```

- `get_parser()` lanza `FileNotFoundError` si el archivo no existe
- `get_parser()` lanza `UnsupportedFormatError` si ning√∫n parser acepta el archivo

---

### Chunker ‚Äî Pasada 1: BoundaryDetector (`tenlib/processor/chunker/detector.py`)

```python
class BoundaryDetector:
    def detect(self, text: str, source_section: int = 0) -> list[TextSegment]
```

- `assert isinstance(text, str)` ‚Äî falla expl√≠cita con bytes u otros tipos
- Texto vac√≠o o s√≥lo whitespace ‚Üí `[]`
- Recorre l√≠nea a l√≠nea clasificando cada una seg√∫n jerarqu√≠a
- Segmento actual acumula l√≠neas hasta encontrar nueva frontera
- El √∫ltimo segmento siempre se incluye (no se pierde)

**Clasificaci√≥n de l√≠neas (jerarqu√≠a descendente):**
1. `BoundaryType.CHAPTER` ‚Äî patrones de cap√≠tulo del `ChunkConfig`
2. `BoundaryType.SCENE` ‚Äî separadores `***`, `---`, `‚Ä¢‚Ä¢‚Ä¢`, `* * *`, `###`
3. `BoundaryType.POV` ‚Äî l√≠neas en MAY√öSCULAS o entre `*asteriscos*`
4. `BoundaryType.PARAGRAPH` ‚Äî **patr√≥n actualmente inoperativo** (requiere whitespace que `stripped` elimina)
5. `BoundaryType.SENTENCE` ‚Äî **patr√≥n actualmente inoperativo** (mismo motivo)

> **Nota t√©cnica:** PARAGRAPH y SENTENCE son dead code en `_classify_line` porque la comparaci√≥n se hace contra `line.strip()`. Los patrones de estas dos clases requieren whitespace inicial que no existe en texto stripeado. Solo operan CHAPTER, SCENE y POV.

**Doble l√≠nea vac√≠a:** tres newlines consecutivos (`\n\n\n`) producen un `BoundaryType.SCENE`, no dos.

---

### Chunker ‚Äî Pasada 2: ChunkNormalizer (`tenlib/processor/chunker/normalizer.py`)

```python
class ChunkNormalizer:
    def normalize(self, segments: list[TextSegment]) -> list[Chunk]
```

**Fase de expansi√≥n** (segmentos > max_tokens):
1. Dividir por p√°rrafos (doble salto de l√≠nea)
2. Si un p√°rrafo sigue siendo > max_tokens: dividir por oraciones (`. `, `? `, `! `)
3. Si una oraci√≥n individual supera max_tokens: se mantiene como chunk √∫nico (sin romper)

**Fase de fusi√≥n** (segmentos < min_tokens):
- Condiciones para fusionar con el anterior:
  1. El segmento anterior tiene < min_tokens
  2. La suma no supera max_tokens
  3. Ninguno de los dos es CHAPTER
- Cap√≠tulos NUNCA se fusionan (preservan estructura del libro)

**Conversi√≥n a Chunk:**
- √çndices secuenciales desde 0
- `status = ChunkStatus.PENDING`
- `source_section` propagado desde el TextSegment

---

### Chunker ‚Äî Coordinador (`tenlib/processor/chunker/chunker.py`)

```python
class Chunker:
    def __init__(self, config: ChunkConfig | None = None,
                 estimator: TokenEstimator | None = None)
    def chunk(self, book: RawBook) -> list[Chunk]
```

- Instancia `BoundaryDetector` y `ChunkNormalizer` internamente
- Itera `book.sections`, llama `detector.detect(section, source_section=i)`
- Concatena todos los `TextSegment` de todas las secciones
- Llama `normalizer.normalize(all_segments)`
- Re-indexa los chunks globalmente desde 0

**TokenEstimator:**
```python
class SimpleTokenEstimator:
    def estimate(self, text: str) -> int:
        return int(len(text.split()) * 1.3)
    # ¬±10% para ingl√©s/espa√±ol ‚Äî v√°lido para el MVP
```

---

### Router (`tenlib/router/router.py`)

```python
class Router:
    def __init__(self, models: list[BaseModel])   # ValueError si lista vac√≠a
    def translate(self, chunk: str, system_prompt: str) -> ModelResponse
    def available_models(self) -> list[str]
```

**Algoritmo de `translate()`:**
```
Para cada modelo en orden de priority:
    Si model.is_available() == False ‚Üí skip
    try:
        return model.translate(chunk, system_prompt)
    except BadRequestError | InvalidArgument | ValueError:
        log "content error"
        re-raise                         ‚Üê sin failover
    except Exception:
        log "retryable error"
        continue al siguiente modelo

raise AllModelsExhaustedError
```

**`_is_content_error(e)`:** devuelve `True` para `anthropic.BadRequestError`, `google.api_core.exceptions.InvalidArgument`, `ValueError`. Estos no hacen failover porque el problema es el contenido, no el modelo.

```python
class AllModelsExhaustedError(Exception): ...
```

---

### Model Adapters

#### ClaudeAdapter (`tenlib/router/claude.py`)

```python
class ClaudeAdapter(BaseModel):
    def __init__(self, config: ModelConfig, repo: Repository)
```

- Modelo: `claude-haiku-4-5-20251001`
- `max_tokens`: 4096
- Env√≠a: `system=system_prompt`, `messages=[{"role": "user", "content": chunk}]`
- Errors retryables ‚Üí cooldown de 5min (`_config._unavailable_until = time.time() + 300`)
- Registra tokens en `repo.add_token_usage()` despu√©s de cada llamada exitosa

**`is_available()`:**
1. Si `_unavailable_until` est√° seteado y no ha pasado ‚Üí False
2. Si ha pasado ‚Üí limpiar `_unavailable_until`
3. `repo.get_token_usage_today(name) < config.daily_token_limit`

#### GeminiAdapter (`tenlib/router/gemini.py`)

- Modelo: `gemini-2.0-flash`
- Configura `response_mime_type: "application/json"` en `GenerationConfig` (formato nativo de Gemini)
- Extrae tokens de `response.usage_metadata.prompt_token_count` / `candidates_token_count`
- Misma l√≥gica de cooldown y quota que Claude
- **FutureWarning activo**: `google-generativeai` est√° deprecado en favor de `google.genai`. No afecta funcionalidad pero es deuda t√©cnica.

---

### Prompt Builder (`tenlib/router/prompt_builder.py`)

```python
def build_translate_prompt(
    source_lang: str,
    target_lang: str,
    voice:      str           = "narrador en tercera persona, tiempo pasado",
    decisions:  list[str]     = None,
    glossary:   dict          = None,
    characters: dict          = None,
    last_scene: Optional[str] = None,
) -> str
```

**El chunk viaja como mensaje de usuario, NO en el system prompt.** Esto mejora la adherencia a reglas en todos los modelos.

**Estructura del prompt:**
1. Rol del modelo (editor literario experto)
2. Contexto de obra: `source_lang`, `target_lang`, `voice`
3. Book Bible (actualmente placeholders con defaults):
   - Glosario: `t√©rmino_origen ‚Üí t√©rmino_destino`
   - Decisiones de estilo: lista de strings
   - Personajes: `nombre: descripci√≥n de tono`
   - Continuidad: escena anterior
4. Instrucciones de salida: JSON obligatorio

**Schema JSON obligatorio para el modelo:**
```json
{
  "notes": "an√°lisis de retos y decisiones (PRIMERO ‚Äî CoT)",
  "confidence": 0.0,
  "translation": "texto traducido completo"
}
```
El orden `notes ‚Üí confidence ‚Üí translation` es intencional: fuerza razonamiento antes de traducir (Chain-of-Thought).

**Fallbacks cuando no se pasan par√°metros opcionales:**
```python
_GLOSSARY_EMPTY   = "Sin glosario todav√≠a ‚Äî extrae t√©rminos relevantes que encuentres."
_DECISIONS_EMPTY  = "Ninguna todav√≠a ‚Äî este es el primer fragmento."
_CHARACTERS_EMPTY = "Sin perfiles definidos todav√≠a ‚Äî infiere el tono de cada personaje del texto."
_LAST_SCENE_EMPTY = "Inicio del libro ‚Äî no hay contexto previo."
```

> **Limitaci√≥n actual:** el prompt recibe siempre los mismos par√°metros globales. La compresi√≥n de contexto por chunk (Book Bible selectiva) es Fase 2 y no est√° implementada.

---

### Response Parser (`tenlib/router/response_parser.py`)

```python
def parse_model_response(raw_text: str, model_name: str) -> dict
# Nunca lanza excepci√≥n. Siempre devuelve {"translation", "confidence", "notes"}
```

**Estrategia de degradaci√≥n:**
1. Parse JSON directo ‚Üí `json.loads(raw_text)`
2. Extraer de bloque markdown ` ```json ... ``` ` o ` ``` ... ``` `
3. Buscar cualquier `{...}` en el texto con regex
4. Modo emergencia: `translation = raw_text`, `confidence = 0.3`, `notes = "ADVERTENCIA: respuesta no parseable"`

**Normalizaci√≥n:**
- `confidence` clampeado a `[0.0, 1.0]`: `max(0.0, min(1.0, value))`
- Claves faltantes: `confidence` default `0.5`, `notes` default `"(sin notas)"`

---

### Repository (`tenlib/storage/repository.py`)

```python
class Repository:
    def __init__(self, db_path: str | None = None)

    # Books
    def create_book(title, file_hash, mode, source_lang, target_lang) -> int
    def get_book_by_hash(file_hash: str) -> StoredBook | None
    def get_book_by_id(book_id: int) -> StoredBook | None
    def update_book_status(book_id: int, status: BookStatus) -> None

    # Chunks
    def save_chunks(book_id: int, chunks: list) -> None          # INSERT OR IGNORE
    def get_pending_chunks(book_id: int) -> list[StoredChunk]    # ORDER BY chunk_index
    def get_all_chunks(book_id: int) -> list[StoredChunk]        # ORDER BY chunk_index
    def update_chunk_translation(chunk_id, translated, model_used,
                                  confidence, status) -> None    # at√≥mico
    def flag_chunk(chunk_id: int, flags: list[str]) -> None      # ‚Üí FLAGGED + JSON flags

    # Quota
    def add_token_usage(model: str, tokens: int) -> None         # UPSERT por (model, date)
    def get_token_usage_today(model: str) -> int

    def close(self) -> None
```

**Idempotencia de `save_chunks`:** `INSERT OR IGNORE` con `UNIQUE(book_id, chunk_index)` ‚Äî reanudaciones no duplican datos.

---

### Config Loader (`tenlib/router/config_loader.py`)

```python
def load_model_configs(config_path: Optional[str] = None) -> list[ModelConfig]
```

**Orden de b√∫squeda del config:**
1. `config_path` expl√≠cito
2. Variable de entorno `TENLIB_CONFIG_PATH`
3. `~/.tenlib/config.yaml`

**Resoluci√≥n de API keys:** `${VAR_NAME}` ‚Üí `os.environ.get("VAR_NAME")`

**Config YAML completo:**
```yaml
models:
  - name: gemini
    priority: 1
    daily_token_limit: 1000000
    api_key: ${GEMINI_API_KEY}
    timeout_seconds: 60
    temperature: 0.3

  - name: claude
    priority: 2
    daily_token_limit: 100000
    api_key: ${ANTHROPIC_API_KEY}
    timeout_seconds: 60
    temperature: 0.3
```

---

## Suite de tests ‚Äî 147 tests

| Archivo                              | Tests | Qu√© cubre                                      |
|--------------------------------------|-------|------------------------------------------------|
| `test_cli.py`                        | 18    | Validaciones CLI, flujo feliz, errores, stubs  |
| `test_orchestrator.py`               | 9     | Pipeline E2E, reanudaci√≥n, errores, confianza  |
| `processor/parsers/test_parser.py`   | 32    | TxtParser, EpubParser, ParserFactory           |
| `processor/chunker/test_detector.py` | 27    | Detecci√≥n de fronteras, patrones, edge cases   |
| `processor/chunker/test_normalizer.py`| 17   | Expansi√≥n, fusi√≥n, casos l√≠mite de tokens      |
| `processor/chunker/test_integration.py`| 6   | Chunker E2E, sin p√©rdida de contenido          |
| `router/test_router.py`              | 7     | Failover, selecci√≥n, AllModelsExhaustedError   |
| `router/test_prompt_builder.py`      | 8     | Prompt, CoT, fallbacks, glosario, personajes   |
| `router/test_response_parser.py`     | 7     | Degradaci√≥n JSON, clamping, markdown blocks    |
| `storage/test_repository.py`         | 17    | CRUD, idempotencia, quota, ordenamiento        |
| **TOTAL**                            | **147** | **100% green**                               |

---

## Directorios en tiempo de ejecuci√≥n

```
~/.tenlib/
‚îú‚îÄ‚îÄ config.yaml          # configuraci√≥n de modelos (copiar de config.example.yaml)
‚îú‚îÄ‚îÄ tenlib.db            # base de datos SQLite
‚îî‚îÄ‚îÄ output/              # archivos traducidos
    ‚îú‚îÄ‚îÄ el_nombre_del_viento_es.txt
    ‚îî‚îÄ‚îÄ ...
```

---

## Deuda t√©cnica y gaps

### Funcionalidad faltante (Fase 2)

| √çtem                          | Impacto                                                  |
|-------------------------------|----------------------------------------------------------|
| Book Bible / Context Engine   | Sin √©l, cada chunk se traduce sin memoria del libro      |
| Reset de libro ya procesado   | `BookAlreadyDoneError` muestra mensaje pero no resetea   |
| Actualizaci√≥n autom√°tica de glosario | El prompt acepta glosario pero nadie lo llena    |
| Compresi√≥n de contexto        | Siempre se env√≠a la Bible completa (cuando exista)       |

### Deuda t√©cnica menor

| √çtem                          | Archivo                    | Nota                                              |
|-------------------------------|----------------------------|---------------------------------------------------|
| `PARAGRAPH` / `SENTENCE` en detector | `detector.py:_classify_line` | Dead code: `stripped` elimina el whitespace que necesitan |
| `book_processor.py`           | `processor/book_processor.py` | Archivo presente pero no usado en el pipeline  |
| `google-generativeai` deprecado | `gemini.py:6`            | FutureWarning en runtime, migrar a `google.genai` |
| `ChunkStatus.REVIEWED`        | `storage/models.py`        | Enum definido pero nunca se asigna en el pipeline |
| `BookMode.FIX` / `WRITE`      | `storage/models.py`        | Enums definidos pero los comandos son stubs       |
| tabla `bible` en schema       | `db.py`                    | Creada pero nunca se escribe                     |

### Lo que NO hay

- UI (Gradio o cualquier otra)
- Exportaci√≥n a EPUB / DOCX
- Quality Checker / cola de revisi√≥n
- Adaptador GPT/OpenAI (solo Claude y Gemini)
- Soporte para `.docx` (mencionado en README pero no implementado)
- Soporte para `.pdf`
- Soporte para planes "Pro" sin API key
- Versionado de Book Bible

---

## Instalaci√≥n y primer uso

```bash
git clone https://github.com/zeus483/TenLib.git
cd tenlib
python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
pip install -e .       # instala el comando `tenlib` en el entorno

# Configurar modelos
mkdir -p ~/.tenlib
cp config.example.yaml ~/.tenlib/config.yaml
# Editar config.yaml con las API keys

# Variables de entorno (o definirlas en el config directamente)
export ANTHROPIC_API_KEY="sk-ant-..."
export GEMINI_API_KEY="AIza..."

# Ejecutar
tenlib translate --book mi_libro.epub --from en --to es

# Correr los tests
pytest tests/ -v
```
