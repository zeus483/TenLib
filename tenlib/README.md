# TenLib
Es un proyecto open Source en donde busco crear un agente editorial con IA para hacer traducciones, mejorar traducciones, ser copiloto al escribir un libro entre otras cosas, esto nace de mi gusto por la lectura japonesa y las pocas traducciones a novelas ligeras que encuentro y ademas mi gusto por la literatura, ahora si el README hecho por IA.
## Notas del Autor:

No se bien ingles pero lo estoy practicando por lo que si ven errores semanticos en el codigo o en los commits una disculpa de antemano.

# ðŸ“š TenLib

> Un editor literario agÃ©ntico de cÃ³digo abierto. Traduce, corrige y escribe libros completos con IA, preservando coherencia y optimizando el uso de tokens.

---

## Â¿Por quÃ© existe esto?

Los libros tienen entre 80.000 y 150.000 palabras. NingÃºn modelo de IA cabe eso en contexto de una sola vez. Si lo partes sin criterio, pierdes coherencia: los personajes cambian de nombre entre capÃ­tulos, el tono varÃ­a, los modismos se traducen de formas distintas en cada fragmento.

Las soluciones actuales (DeepL, plugins de Calibre, pegar fragmentos en ChatGPT) tratan cada chunk como un texto nuevo, sin memoria del resto del libro. **TenLib resuelve eso** construyendo una memoria editorial persistente que viaja con cada fragmento a lo largo de todo el proceso.

AdemÃ¡s, la mayorÃ­a de personas con acceso a mÃºltiples IAs (Claude Pro, GPT Plus, Gemini Pro) no puede aprovecharlos en conjunto. TenLib los unifica en un solo pipeline con rotaciÃ³n automÃ¡tica cuando se agotan los tokens del dÃ­a.

---

## CaracterÃ­sticas principales

- **Chunking semÃ¡ntico** â€” divide por escenas y capÃ­tulos, no por tamaÃ±o fijo
- **Book Bible** â€” memoria editorial persistente: glosario, personajes, voz narrativa, decisiones de estilo
- **CompresiÃ³n de contexto** â€” solo el contexto relevante viaja en cada llamada (hasta 40% menos tokens)
- **Multi-modelo con rotaciÃ³n** â€” Claude, GPT y Gemini en un solo pipeline con failover automÃ¡tico
- **ReanudaciÃ³n automÃ¡tica** â€” si el proceso se interrumpe, continÃºa desde donde quedÃ³
- **Control de calidad** â€” detector de inconsistencias y cola de revisiÃ³n humana
- **MÃºltiples modos** â€” traducciÃ³n, correcciÃ³n de traducciones, ajuste de estilo, co-autorÃ­a

---

## Modos de operaciÃ³n

```bash
# Traducir un libro de inglÃ©s a espaÃ±ol
libreditor translate --book libro.epub --from en --to es

# Corregir o mejorar una traducciÃ³n existente con el original como referencia
libreditor fix-translation --book traduccion.epub --reference original.epub

# Abrir la interfaz de revisiÃ³n humana para un libro procesado
libreditor review --book mi_libro

# Modo co-autor: desarrollar una idea hasta un libro completo
libreditor write --outline mi_idea.txt
```

---

## Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  GRADIO UI / CLI                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ORCHESTRATOR                       â”‚
â”‚   Gestiona el pipeline Â· Coordina mÃ³dulos           â”‚
â”‚   Controla flujo de chunks Â· Maneja errores         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚               â”‚               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    BOOK     â”‚ â”‚   CONTEXT   â”‚ â”‚      MODEL          â”‚
â”‚  PROCESSOR  â”‚ â”‚   ENGINE    â”‚ â”‚      ROUTER         â”‚
â”‚             â”‚ â”‚             â”‚ â”‚                     â”‚
â”‚ Â· Parse     â”‚ â”‚ Â· Book Bibleâ”‚ â”‚ Â· Claude            â”‚
â”‚ Â· Chunk     â”‚ â”‚ Â· Compress  â”‚ â”‚ Â· GPT               â”‚
â”‚ Â· Reconstructâ”‚ â”‚ Â· Update   â”‚ â”‚ Â· Gemini            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ Â· Token tracker     â”‚
                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                QUALITY CHECKER                      â”‚
â”‚   Detector de inconsistencias Â· Comparador          â”‚
â”‚   Cola de revisiÃ³n humana Â· Marcador de confianza   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STORAGE (SQLite local)                 â”‚
â”‚   books Â· chunks Â· bible Â· quota_usage              â”‚
â”‚   /output â†’ EPUB, DOCX, TXT reconstruidos           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## MÃ³dulos en detalle

### 1. Book Processor

Convierte el libro crudo en chunks procesables y, al final, reconstruye el archivo de salida.

**Chunking semÃ¡ntico:** no divide por cantidad fija de tokens. Detecta primero capÃ­tulos, luego escenas (separadores `***`, saltos dobles, cambios de POV indicados por el autor). Cada chunk queda entre 800 y 2000 tokens â€” suficiente para que el modelo tenga contexto interno, pequeÃ±o para caber junto a la Book Bible en el prompt.

**Formatos soportados (v1):**
- `.txt` â€” texto plano
- `.epub` â€” via `ebooklib`
- `.docx` â€” via `python-docx`

> PDF se deja para versiones futuras por la complejidad del layout.

---

### 2. Context Engine *(el corazÃ³n del sistema)*

Mantiene y administra la **Book Bible**: un objeto JSON vivo que representa la memoria editorial del libro completo.

```json
{
  "meta": {
    "title": "El nombre del viento",
    "source_lang": "en",
    "target_lang": "es",
    "voice": "tercera persona, pasado, tono Ã©pico-intimista",
    "decisions": [
      "tutear al lector",
      "mantener 'Naming' sin traducir",
      "conservar 'Chandrian' en lugar de hispanizarlo"
    ]
  },
  "glossary": {
    "Kvothe": "Kvothe",
    "Sympathy": "SimpatÃ­a",
    "the Chandrian": "los Chandrian",
    "Naming": "Naming"
  },
  "characters": {
    "Kvothe": "protagonista, voz activa, habla directo y sin rodeos",
    "Chronicler": "escriba, tono formal y observador"
  },
  "continuity": {
    "last_scene": "Kvothe acaba de llegar a la Universidad",
    "open_threads": [
      "mencionÃ³ a su madre en cap 3, hilo sin resolver"
    ]
  }
}
```

**Antes de cada chunk:** el motor comprime la Biblia a lo estrictamente relevante para ese fragmento. Si el chunk no contiene al Chronicler, su entrada no va en el prompt. En libros con elencos grandes esto reduce hasta un 40% el uso de tokens.

**DespuÃ©s de cada chunk:** el motor actualiza la Biblia con nuevas decisiones detectadas (tÃ©rminos nuevos, decisiones de estilo tomadas por el modelo, continuity updates).

La Biblia se versiona en SQLite â€” puedes revertir a cualquier estado anterior si una decisiÃ³n automÃ¡tica fue incorrecta.

---

### 3. Model Router

Gestiona los tres modelos de forma transparente. El Orchestrator no sabe ni le importa quÃ© modelo procesÃ³ cada chunk.

**ConfiguraciÃ³n** en `~/.libreditor/quota.yaml`:

```yaml
models:
  - name: claude
    type: api          # 'api' o 'pro' (plan de suscripciÃ³n)
    priority: 1
    daily_token_limit: 100000
    api_key: ${ANTHROPIC_API_KEY}

  - name: gemini
    type: pro
    priority: 2
    daily_token_limit: 80000

  - name: gpt
    type: plus
    priority: 3
    daily_token_limit: 80000
```

**LÃ³gica de rotaciÃ³n:**
1. Intenta el modelo de mayor prioridad disponible
2. Si recibe error 429 (rate limit) o supera el lÃ­mite configurado â†’ pasa al siguiente
3. Si todos estÃ¡n agotados â†’ pausa y notifica al usuario con tiempo estimado de espera
4. Cada chunk registra en SQLite quÃ© modelo lo procesÃ³ (importante para auditorÃ­a y consistencia)

---

### 4. Quality Checker

Corre en paralelo al pipeline principal, no lo bloquea.

**Detecta automÃ¡ticamente:**
- El mismo tÃ©rmino fuente traducido de dos formas distintas (cruza contra el glosario)
- Cambio de tiempo verbal entre chunks consecutivos
- Nombres propios que aparecen sin estar en el glosario (posible error o tÃ©rmino nuevo)
- Fragmentos donde el propio modelo reportÃ³ baja confianza

**Confianza del modelo:** cada llamada al modelo devuelve un JSON estructurado:

```json
{
  "translation": "texto traducido aquÃ­...",
  "confidence": 0.82,
  "notes": "expresiÃ³n idiomÃ¡tica 'under the weather' â€” optÃ© por 'no estar bien', pero podrÃ­a ser 'estar pachuco' segÃºn el registro"
}
```

Los fragmentos con `confidence < 0.75` o con flags del checker van a una **cola de revisiÃ³n humana** visible en la UI.

---

### 5. Storage

Todo en SQLite local. Sin dependencias externas, sin nube obligatoria.

```sql
-- Esquema principal

CREATE TABLE books (
    id INTEGER PRIMARY KEY,
    title TEXT,
    source_lang TEXT,
    target_lang TEXT,
    mode TEXT,           -- 'translate', 'fix', 'write'
    status TEXT,         -- 'in_progress', 'review', 'done'
    created_at TIMESTAMP
);

CREATE TABLE chunks (
    id INTEGER PRIMARY KEY,
    book_id INTEGER,
    chunk_index INTEGER,
    original TEXT,
    translated TEXT,
    model_used TEXT,
    confidence REAL,
    status TEXT,         -- 'pending', 'done', 'flagged', 'reviewed'
    FOREIGN KEY (book_id) REFERENCES books(id)
);

CREATE TABLE bible (
    id INTEGER PRIMARY KEY,
    book_id INTEGER,
    version INTEGER,
    content_json TEXT,
    updated_at TIMESTAMP
);

CREATE TABLE quota_usage (
    model TEXT,
    date TEXT,
    tokens_used INTEGER,
    PRIMARY KEY (model, date)
);
```

**ReanudaciÃ³n automÃ¡tica:** si el proceso se interrumpe, al reanudarlo el Orchestrator consulta `WHERE status = 'pending'` y continÃºa desde ahÃ­. No se reprocesa nada.

---

## Roadmap de desarrollo

El proyecto se construye en 4 fases para tener algo funcional desde el primer sprint.

### Fase 1 â€” MVP (1-2 semanas)
> Objetivo: pipeline funcional de extremo a extremo con un modelo

- [ ] Book Processor: parse de TXT y EPUB, chunking semÃ¡ntico bÃ¡sico
- [ ] Llamada a un modelo (Claude API) con prompt de traducciÃ³n
- [ ] ReconstrucciÃ³n del archivo de salida en TXT
- [ ] Storage SQLite bÃ¡sico (books + chunks)
- [ ] CLI mÃ­nimo: `libreditor translate --book X --from en --to es`

**Criterio de Ã©xito:** traducir un libro completo de 100.000 palabras de principio a fin, con output coherente y reanudable.

---

### Fase 2 â€” Context Engine (1-2 semanas)
> Objetivo: la Book Bible entra en el pipeline

- [ ] Estructura JSON de la Book Bible
- [ ] ExtracciÃ³n automÃ¡tica de glosario en el primer chunk
- [ ] CompresiÃ³n de contexto por chunk
- [ ] ActualizaciÃ³n incremental de la Biblia
- [ ] Versionado de la Biblia en SQLite

**Criterio de Ã©xito:** mismo libro traducido en Fase 1, ahora con consistencia de nombres y tÃ©rminos a lo largo de todo el texto.

---

### Fase 3 â€” Model Router (1 semana)
> Objetivo: los tres modelos en un solo pipeline

- [ ] AbstracciÃ³n unificada de llamadas (Claude / GPT / Gemini)
- [ ] ConfiguraciÃ³n de quota por modelo en YAML
- [ ] RotaciÃ³n automÃ¡tica con failover
- [ ] Tracking de tokens en SQLite
- [ ] Soporte para planes Pro (sin API key) vÃ­a automatizaciÃ³n ligera

**Criterio de Ã©xito:** procesar un libro usando los tres modelos en rotaciÃ³n sin intervenciÃ³n manual.

---

### Fase 4 â€” Quality + UI (2 semanas)
> Objetivo: producto completo y usable por otros

- [ ] Quality Checker con detecciÃ³n de inconsistencias
- [ ] Cola de revisiÃ³n humana
- [ ] UI Gradio: progreso en tiempo real, revisiÃ³n de chunks, ediciÃ³n de la Biblia
- [ ] Modo `fix-translation` (correcciÃ³n de traducciÃ³n existente)
- [ ] Modo `write` (co-autorÃ­a con outline)
- [ ] ExportaciÃ³n a EPUB y DOCX
- [ ] DocumentaciÃ³n de usuario

---

## Stack tecnolÃ³gico

| Componente | TecnologÃ­a |
|---|---|
| Lenguaje | Python 3.11+ |
| UI | Gradio |
| Storage | SQLite (via `sqlite3` stdlib) |
| Parse EPUB | `ebooklib` |
| Parse DOCX | `python-docx` |
| Claude | `anthropic` SDK |
| GPT | `openai` SDK |
| Gemini | `google-generativeai` SDK |
| CLI | `click` |
| Config | `PyYAML` |

---

## InstalaciÃ³n (Fase 1)

```bash
git clone https://github.com/tu-usuario/libreditor.git
cd libreditor

python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt

# Configurar modelos
cp config.example.yaml ~/.libreditor/quota.yaml
# Editar el archivo con tus API keys o configuraciÃ³n de planes Pro
```

---

## Estructura del proyecto

```
libreditor/
â”œâ”€â”€ libreditor/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                  # Punto de entrada CLI
â”‚   â”œâ”€â”€ orchestrator.py         # Coordinador del pipeline
â”‚   â”œâ”€â”€ processor/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ parser.py           # Parse de EPUB, DOCX, TXT
â”‚   â”‚   â””â”€â”€ chunker.py          # Chunking semÃ¡ntico
â”‚   â”œâ”€â”€ context/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ bible.py            # Book Bible (estructura + versionado)
â”‚   â”‚   â””â”€â”€ compressor.py       # CompresiÃ³n de contexto por chunk
â”‚   â”œâ”€â”€ router/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py             # Interfaz abstracta de modelo
â”‚   â”‚   â”œâ”€â”€ claude.py
â”‚   â”‚   â”œâ”€â”€ gpt.py
â”‚   â”‚   â””â”€â”€ gemini.py
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ checker.py          # DetecciÃ³n de inconsistencias
â”‚   â”œâ”€â”€ storage/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ db.py               # SQLite + queries
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ app.py              # Gradio UI
â”œâ”€â”€ tests/
â”œâ”€â”€ config.example.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Principios de diseÃ±o

**Local-first.** Todos los datos del libro, la Biblia y el progreso viven en tu mÃ¡quina. NingÃºn dato sale salvo las llamadas a los modelos que tÃº mismo configuras.

**Reanudable por defecto.** Cualquier proceso puede interrumpirse y retomarse. El estado siempre estÃ¡ en disco.

**Modelo-agnÃ³stico.** Agregar un modelo nuevo es implementar una clase que hereda de `BaseModel`. El resto del sistema no cambia.

**La calidad primero.** El objetivo no es traducir rÃ¡pido sino traducir bien. La velocidad es una consecuencia de optimizar tokens, no el fin.

---

## Contribuir

El proyecto estÃ¡ en construcciÃ³n activa. Las contribuciones mÃ¡s valiosas en este momento son:

- Parsers para nuevos formatos (PDF, RTF, ODT)
- Adaptadores para nuevos modelos
- Mejoras al algoritmo de chunking semÃ¡ntico
- Prompts de sistema mejor calibrados para distintos gÃ©neros literarios

Abre un issue antes de un PR grande para alinear direcciÃ³n.

---

## Licencia

MIT â€” libre para usar, modificar y distribuir.

---

*TenLib naciÃ³ de la frustraciÃ³n de leer libros en traducciones mediocres cuando la tecnologÃ­a para hacerlo mejor ya existe. Solo faltaba juntarla bien.*